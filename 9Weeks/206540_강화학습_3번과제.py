# -*- coding: utf-8 -*-
"""206540_ê°•í™”í•™ìŠµ_3ë²ˆê³¼ì œ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BksOJ1Dtys89deVv-5sknBNJJ6CNsBzV
"""

import gym #gym í™˜ê²½ ë¶ˆëŸ¬ì˜¤ê¸°
import numpy as np #numpy ì“°ê¸° ê·€ì°®ì•„ì„œ np
from time import sleep #sleep ì“¸ë ¤ê³  timeì—ì„œ ê°€ì ¸ì˜´
from IPython.display import display, clear_output, Pretty #ì»´íŒŒì¼í• ë•Œ ë³´ì´ë„ë¡ í• ë ¤ê³  ì´ëŸ°ê±° ì”€

env = gym.make('FrozenLake8x8-v0', is_slippery=False) # ì–¼ìŒìœ„ì—ì„œ ë¯¸ë„ëŸ¬ì§€ì§€ ì•Šë„ë¡ ì„¤ì •
#env = gym.make('FrozenLake8x8-v0') #ë¯¸ë„ëŸ½ê²Œ í• ë ¤ê³  í• ë•Œì—ëŠ” ì´ë ‡ê²Œ í•˜ë©´ ë¨
state = env.reset() # stateì— ì´ˆê¸°ìƒíƒœ ì €ì¥

# Initial world display
world = env.render(mode='ansi') 
display(Pretty(world))
sleep(0.5)

"""Q-Learning
Q-Learning ì˜ ì›ë¦¬ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì •ì±…(policy)ìœ¼ë¡œ í•™ìŠµ ì‹œí‚¨ ë°ì´í„°ë¥¼ ì„ì–´ë„ ìµœì í™”ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.

SARSA + greedy
SARSA + greedy ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œ Q-valueëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

Q(St,Aâ€²t)â†Q(St,Aâ€²t)+Î±(Rt+1+Î³Q(St+1,Aâ€²t+1)âˆ’Q(St,Aâ€²t))
greedy ë°©ì‹ì€ ë‹¤ìŒ ìŠ¤í…ì˜ ì•¡ì…˜ì„ ì„ íƒí•˜ëŠ” ê²½ìš°ì— ìš•ì‹¬ìŸì´(greedy)ì²˜ëŸ¼ ìµœê³ ì˜ Q-valueì˜ ì•¡ì…˜ë§Œì„ ì„ íƒí•œë‹¤. ë”°ë¼ì„œ ë‹¤ìŒ ì‹ì„ ë§Œì¡±í•œë‹¤.

Q(St+1,Aâ€²t+1)=maxaâ€²Q(St+1,aâ€²)
ë”°ë¼ì„œ Q-valueëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

Q(St,Aâ€²t)â†Q(St,Aâ€²t)+Î±(Rt+1+Î³maxaâ€²Q(St+1,aâ€²)âˆ’Q(St,Aâ€²t))
SARSA + ğœ€-greedy
SARSA + ğœ€-greedy ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œ Q-valueëŠ” ì•„ë˜ì™€ ê°™ë‹¤.

Q(St,At)â†Q(St,At)+Î±(Rt+1+Î³Q(St+1,At+1)âˆ’Q(St,At))
ì—¬ê¸°ì„œ Q(St+1,At+1) ëŒ€ì‹ ì— SARSA + greedy ë°©ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ maxaâ€²Q(St+1,aâ€²)ì„ ì‚¬ìš©í•œë‹¤. ê·¸ëŸ¬ë©´â€¦

Q-Learning
Q(St,At)â†Q(St,At)+Î±(Rt+1+Î³maxaâ€²Q(St+1,aâ€²)âˆ’Q(St,At))
Î± : learning rate
Î³ : ë””ìŠ¤ì¹´ìš´íŠ¸ (discount factor)

Target
ëª©í‘œë¡œ í•˜ëŠ” ê°’ì´ë‹¤. ì—¬ê¸°ì„œ íƒ€ê²Ÿì€ Rt+1+Î³maxaâ€²Q(St+1,aâ€²)ì´ë‹¤.

Error (ë¸íƒ€)
ëª©í‘œê°’ê³¼ í˜„ì¬ê°’ê³¼ì˜ ì°¨ì´ë¥¼ Î´ ë¼ê³  í•œë‹¤.

Î´t=Rt+1+Î³maxaâ€²Q(St+1,aâ€²)âˆ’Q(St,At)
ê³„ì†í•´ì„œ ì—í”¼ì†Œë“œë¥¼ ì‹¤í–‰ ì‹œí‚¤ë©´ì„œ Q(St,At)ì—ë‹¤ê°€ Î±âˆ—Î´t ë¥¼ ì—…ë°ì´íŠ¸ í•˜ë©´ ê²°êµ­ Q(s,a)â†’qâˆ—(s,a)ê°€ ëœë‹¤.
"""

from tqdm import tqdm

num_state = env.observation_space.n
num_action = env.action_space.n
num_episode = 5000

# Initialize Q_table 
Q_table = np.random.uniform(low=0.0, high=0.00000001, size=(num_state, num_action))
# Zero for terminate states
for s in [19, 29, 35, 41, 42, 49, 52, 54, 59, 63]:
    Q_table[s] = 0

# Hyper parameter
epsilon = 0.3
alpha = 0.1
gamma = 0.9

for episode in tqdm(range(num_episode)):
    state = env.reset()
    done = False
    while not done:
        if np.random.uniform() < epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(Q_table[state])
        next_state, reward, done, info = env.step(action)
        
        target = reward + gamma*Q_table[next_state, np.argmax(Q_table[next_state])] 
        delta = target - Q_table[state][action]
        Q_table[state][action] += alpha * delta
        state = next_state

state = env.reset()
done = False

# Initial world display
world = env.render(mode='ansi')
display(Pretty(world))
sleep(0.5)

while not done:
    action = np.argmax(Q_table[state]) # Optimal Policy
    state, reward, done, info = env.step(action)
    
    # updated world display
    world = env.render(mode='ansi')
    clear_output(wait=True)
    display(Pretty(world))
    sleep(0.5)
    
    if done and state == 63:
        print('\n ë„ì°©! ')