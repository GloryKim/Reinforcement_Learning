# -*- coding: utf-8 -*-
"""206540_강화학습_3번과제.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BksOJ1Dtys89deVv-5sknBNJJ6CNsBzV
"""

import gym #gym 환경 불러오기
import numpy as np #numpy 쓰기 귀찮아서 np
from time import sleep #sleep 쓸려고 time에서 가져옴
from IPython.display import display, clear_output, Pretty #컴파일할때 보이도록 할려고 이런거 씀

env = gym.make('FrozenLake8x8-v0', is_slippery=False) # 얼음위에서 미끄러지지 않도록 설정
#env = gym.make('FrozenLake8x8-v0') #미끄럽게 할려고 할때에는 이렇게 하면 됨
state = env.reset() # state에 초기상태 저장

# Initial world display
world = env.render(mode='ansi') 
display(Pretty(world))
sleep(0.5)

"""Q-Learning
Q-Learning 의 원리는 서로 다른 정책(policy)으로 학습 시킨 데이터를 섞어도 최적화가 가능하다는 것이다.

SARSA + greedy
SARSA + greedy 방식으로 학습한 Q-value는 아래와 같다.

Q(St,A′t)←Q(St,A′t)+α(Rt+1+γQ(St+1,A′t+1)−Q(St,A′t))
greedy 방식은 다음 스텝의 액션을 선택하는 경우에 욕심쟁이(greedy)처럼 최고의 Q-value의 액션만을 선택한다. 따라서 다음 식을 만족한다.

Q(St+1,A′t+1)=maxa′Q(St+1,a′)
따라서 Q-value는 다음과 같다.

Q(St,A′t)←Q(St,A′t)+α(Rt+1+γmaxa′Q(St+1,a′)−Q(St,A′t))
SARSA + 𝜀-greedy
SARSA + 𝜀-greedy 방식으로 학습한 Q-value는 아래와 같다.

Q(St,At)←Q(St,At)+α(Rt+1+γQ(St+1,At+1)−Q(St,At))
여기서 Q(St+1,At+1) 대신에 SARSA + greedy 방식으로 만들어진 maxa′Q(St+1,a′)을 사용한다. 그러면…

Q-Learning
Q(St,At)←Q(St,At)+α(Rt+1+γmaxa′Q(St+1,a′)−Q(St,At))
α : learning rate
γ : 디스카운트 (discount factor)

Target
목표로 하는 값이다. 여기서 타겟은 Rt+1+γmaxa′Q(St+1,a′)이다.

Error (델타)
목표값과 현재값과의 차이를 δ 라고 한다.

δt=Rt+1+γmaxa′Q(St+1,a′)−Q(St,At)
계속해서 에피소드를 실행 시키면서 Q(St,At)에다가 α∗δt 를 업데이트 하면 결국 Q(s,a)→q∗(s,a)가 된다.
"""

from tqdm import tqdm

num_state = env.observation_space.n
num_action = env.action_space.n
num_episode = 5000

# Initialize Q_table 
Q_table = np.random.uniform(low=0.0, high=0.00000001, size=(num_state, num_action))
# Zero for terminate states
for s in [19, 29, 35, 41, 42, 49, 52, 54, 59, 63]:
    Q_table[s] = 0

# Hyper parameter
epsilon = 0.3
alpha = 0.1
gamma = 0.9

for episode in tqdm(range(num_episode)):
    state = env.reset()
    done = False
    while not done:
        if np.random.uniform() < epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(Q_table[state])
        next_state, reward, done, info = env.step(action)
        
        target = reward + gamma*Q_table[next_state, np.argmax(Q_table[next_state])] 
        delta = target - Q_table[state][action]
        Q_table[state][action] += alpha * delta
        state = next_state

state = env.reset()
done = False

# Initial world display
world = env.render(mode='ansi')
display(Pretty(world))
sleep(0.5)

while not done:
    action = np.argmax(Q_table[state]) # Optimal Policy
    state, reward, done, info = env.step(action)
    
    # updated world display
    world = env.render(mode='ansi')
    clear_output(wait=True)
    display(Pretty(world))
    sleep(0.5)
    
    if done and state == 63:
        print('\n 도착! ')